from typing import Type, Optional, Dict, List
from abc import ABC, abstractmethod
import os
from pathlib import Path
import instructor
import ollama
from openai import OpenAI
from pydantic import BaseModel
from pydantic_settings import BaseSettings
from loguru import logger

from LLM.output import Output
from dotenv import load_dotenv

class LLMConfig(BaseSettings):
    """Configuration for LLM models. Only contains non-sensitive settings."""
    ollama_host: str = "http://localhost:11434"
    timeout: int = 300
    max_retries: int = 3
    
    class Config:
        token_file = ".token"
        extra = "ignore"  # Ignore extra fields

def get_api_key(key_name: str) -> Optional[str]:
    """Get API key from environment variables or .env file."""
    
    # Get the directory of the current file
    current_dir = Path(__file__).parent
    env_path = current_dir / '.env'
    
    # Load .env file if it exists
    if env_path.exists():
        load_dotenv(env_path)
    
    return os.getenv(key_name)

class LModel(ABC):
    """Base class for LLM implementations."""
    
    def __init__(self, model_name: str, config: Optional[LLMConfig] = None):
        self.config = config or LLMConfig()
        self.model_name = model_name
        self.messages_memory: List[Dict[str, str]] = []
        self._initialize_client()

    @abstractmethod
    def _initialize_client(self) -> None:
        """Initialize the model client."""
        pass

    @abstractmethod
    def execute(self, messages: List[Dict[str, str]], 
                schema_model: Optional[Type[BaseModel]] = None) -> BaseModel:
        """Execute model inference."""
        pass

    def query(self, message: str, output_format: Optional[Type[BaseModel]] = None,
              remember: bool = True) -> BaseModel:
        """Process a query with optional output format validation."""
        tmp_messages = self.messages_memory.copy()
        tmp_messages.append({"role": "user", "content": message})
        
        result = self.execute(tmp_messages, output_format)
        
        if remember:
            self.messages_memory.extend([
                {"role": "user", "content": message},
                {"role": "assistant", "content": result.model_dump_json()}
            ])
        
        return result

    def add_message(self, role: str, content: str):
        """
        Add a message to the messages list with a specified role and content.

        This method appends a dictionary containing the role and content of a message
        to the `messages` list. The role must be one of the predefined valid roles:
        "user", "system", or "assistant". If an invalid role is provided, a
        `ValueError` is raised.

        :param role: The role associated with the message. Must be "user", "system", or "assistant".
        :type role: str
        :param content: The content of the message.
        :type content: str

        :raises ValueError: If the role is not one of "user", "system", or "assistant".
        """
        if not role in ["user", "system", "assistant"]:
            raise ValueError(f"Invalid role: {role}")
        self.messages_memory.append({"role": role, "content": content})


    def query(self, message: str, output_format: Type[BaseModel] = None, remember=True) -> str:
        """
        Process a user message and obtain a response from the assistant.

        This method appends the user's message to a temporary copy of the
        message memory, executes a query to get a response, and optionally
        remembers the interaction by updating the main message memory.

        :param output_format:
        :param message: The message from the user to be processed.
        :type message: str
        :param remember: A flag indicating whether to store the interaction
                         in the message memory. Defaults to True.
        :type remember: bool
        :return: The response generated by the assistant.
        :rtype: str
        """
        # Create a temporary copy of the message memory
        tmp_messages_memory = self.messages_memory.copy()
        # Add the user's message to the temporary memory
        tmp_messages_memory.append({"role": "user", "content": message})
        # Execute the query and get the result
        result = self.execute(tmp_messages_memory, output_format).result

        if remember:
            # Remember the interaction by updating the main message memory
            self.messages_memory.append({"role": "user", "content": message})
            self.messages_memory.append({"role": "assistant", "content": result})

        return result


    def query_json(self, message: str, output_format: Type[BaseModel] = None, remember=True) -> BaseModel | None:
        """
        Process a user message and obtain a JSON-formatted response.

        :param message: The input message from the user
        :param output_format: The expected output format, should be a subclass of BaseModel
        :param remember: Whether to remember this interaction, defaults to True
        :return: A validated BaseModel instance, or None if validation fails
        """
        # Create a copy of the temporary message list
        tmp_messages_memory = self.messages_memory.copy()
        # Add the user message to the temporary list
        tmp_messages_memory.append({"role": "user", "content": message})
        # Insert the system prompt at the beginning of the list to guide AI in generating JSON output
        tmp_messages_memory.insert(0, {"role": "system",
                                       "content": self.get_json_prompt(output_format)})
        # Execute the query and get the result
        result = self.execute(tmp_messages_memory, output_format)

        if remember:
            # Remember the interaction by updating the main message memory
            self.messages_memory.append({"role": "user", "content": message})
            self.messages_memory.append({"role": "assistant", "content": result.model_dump_json()})
        return result


    def forget_last_query(self):
        """
        Forget the last user-assistant interaction by removing the last two messages.
        """
        self.messages_memory.pop()
        self.messages_memory.pop()

    def set_system_prompt(self, message):
        """
        Set a system prompt message.
        :param message: The system prompt message to be added.
        :type message: str
        """
        self.messages_memory.append({"role": "system", "content": message})

    @staticmethod
    def class_generator(model_name: str):
        """
        Generate a class instance based on the model name.
        :param model_name: The name of the model.
        :type model_name: str
        :return: An instance of the corresponding model class.
        :rtype: LModel
        """
        class_dict = {
            "gpt": OpenAIModel,
            "qwen": QwenModelLocal,
            "llama": OllamaModel,
            "deepseek": DeepseekModel,
            "deepseek-r1": DeepseekModelLocal,
        }
        # Iterate over each key in the dictionary
        for key in class_dict:
            # Check if the current key is a substring of the model_client string
            if key in model_name:
                # If found, return the corresponding value from the dictionary
                return class_dict[key](model_name)
        raise ValueError(f"Unknown model name: {model_name}")

    @classmethod
    def get_short_name(cls,model_client:str) -> str:
        """
        Get the short name of the model client.
        :param model_client: The name of the model client.
        :type model_client: str
        :return: The short name of the model client.
        :rtype: str
        """
        # Define a dictionary mapping model names to their short forms
        class_dict = {
            "gpt": "gpt",
            "qwen2.5": "qwen2.5",
            "qwen": "qwen",
            "llama": "llama",
            "deepseek-chat": "deepseek",
            "deepseek-r1": "deepseek-r1",
        }

        # Iterate over each key in the dictionary
        for key in class_dict:
            # Check if the current key is a substring of the model_client string
            if key in model_client:
                # If found, return the corresponding value from the dictionary
                return class_dict[key]

        # If no keys are found in model_client, raise an exception
        raise ValueError(f"Unknown model name: {model_client}")

    def re_init_chat(self, system_messages:str=None):
        """
        Re-initialize the chat with a given system message.

        This method clears the current list of messages and appends a new system
        message. If no system message is provided, it defaults to using the first
        message in the existing list if it has a role of "system".

        :param system_messages: Optional; A dictionary representing the system
                                message to initialize the chat with. If None,
                                defaults to the first message in the list if it
                                is a system message.
        :type system_messages: dict or None
        """
        # Check if the first message is a system message and assign it if not already set
        if system_messages is None and self.messages_memory and self.messages_memory[0]["role"] == "system":
            system_messages = self.messages_memory[0]

        # Clear all messages
        self.messages_memory.clear()

        # Append the system message if it exists
        if system_messages:
            logger.info("Renew LLM system message.")
            self.messages_memory.append(system_messages)

    @staticmethod
    def get_json_prompt(output_format: Type[BaseModel]) -> str:
        """
        Get the JSON prompt for the given output format.
        :param output_format: The expected output format, should be a subclass of BaseModel.
        :type output_format: Type[BaseModel]
        :return: The JSON prompt string.
        :rtype: str
        """
        return rf"""
        You will get provided a JSON schema responses of a Pydantic model. 
        Your task is to extract those in this JSON schema specified properties out of a given text,
         and return a VALID JSON response adhering to the JSON schema.
        Your Answer must not contain (```json**\n).
        Here is the JSON schema: {output_format.model_json_schema()}.
        You WILL return the instance of the JSON schema with the CORRECT extracted data, NOT the JSON schema itself. 
        """.strip()

    @abstractmethod
    def close(self):
        """
        Close the client connection.
        """
        raise AttributeError('Sub class does not implement this function.')

    @abstractmethod
    def execute(self, messages:list, schema_model: Type[BaseModel] = None) -> BaseModel:
        """
        Execute the query with the given messages and schema model.
        :param messages: The list of messages to be processed.
        :type messages: list
        :param schema_model: The expected output format, should be a subclass of BaseModel.
        :type schema_model: Type[BaseModel]
        :return: The response from the assistant.
        :rtype: BaseModel
        """
        raise AttributeError('Sub class does not implement this function.')


class OpenAIModel(LModel):
    """OpenAI API implementation."""
    
    def _initialize_client(self) -> None:
        api_key = get_api_key("OPENAI")
        if not api_key:
            raise ValueError("OpenAI API key not found in environment variables")
        self.client = OpenAI(api_key=api_key)
    
    def execute(self, messages: List[Dict[str, str]], 
                schema_model: Optional[Type[BaseModel]] = None) -> BaseModel:
        for attempt in range(self.config.max_retries):
            try:
                completion = self.client.beta.chat.completions.parse(
                    model=self.model_name,
                    messages=messages,
                    response_format=schema_model or Output.StructureAnswer,
                    timeout=self.config.timeout
                )
                return (schema_model or Output.StructureAnswer).model_validate_json(
                    completion.choices[0].message.content
                )
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    raise e
                logger.warning(f"Attempt {attempt + 1} failed: {e}")

    def close(self):
        self.client.close()
        logger.info("Connection closed")


class OllamaModel(LModel):
    """Ollama API implementation."""
    
    def _initialize_client(self) -> None:
        try:
            self.client = ollama.Client(host=self.config.ollama_host)
        except Exception as e:
            logger.warning(f"init ollama failed: {e}")
            raise ValueError("init ollama failed")
    
    def execute(self, messages: List[Dict[str, str]], 
                schema_model: Optional[Type[BaseModel]] = None) -> BaseModel:
        for attempt in range(self.config.max_retries):
            try:
                completion = self.client.chat(
                    model=self.model_name,
                    messages=messages,
                    options={"num_ctx": 5120, "timeout": self.config.timeout},
                    format=(schema_model or Output.StructureAnswer).model_json_schema(),
                )
                return (schema_model or Output.StructureAnswer).model_validate_json(
                    completion['message']['content']
                )
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    raise e
                logger.warning(f"Attempt {attempt + 1} failed: {e}")

    def close(self):
        """
        Clean up the Ollama client connection.
        Attempts to gracefully close the connection and provides CLI instructions
        if the programmatic close fails.
        """
        try:
            # Attempt to clean up the client instance
            if hasattr(self.client, '_session'):
                self.client._session.close()
            self.client = None
            logger.info("Ollama client connection closed.")
        except Exception as e:
            logger.warning(f"Failed to close Ollama client programmatically: {e}")
            logger.info("To completely stop Ollama, run in terminal:")
            logger.info("  ollama serve --stop")

class QwenModelLocal(LModel):
    """Qwen API implementation."""
    
    def _initialize_client(self) -> None:
        try:
            self.client = ollama.Client(host=self.config.ollama_host)
        except Exception as e:
            logger.warning(f"init qwen failed: {e}")
            raise ValueError("init qwen failed")
    
    def execute(self, messages: List[Dict[str, str]], 
                schema_model: Optional[Type[BaseModel]] = None) -> BaseModel:
        for attempt in range(self.config.max_retries):
            try:
                completion = self.client.chat(
                    model=self.model_name,
                    messages=messages,
                    options={"num_ctx": 5120, "timeout": self.config.timeout},
                    format=(schema_model or Output.StructureAnswer).model_json_schema(),
                )
                return (schema_model or Output.StructureAnswer).model_validate_json(
                    completion['message']['content']
                )
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    raise e
                logger.warning(f"Attempt {attempt + 1} failed: {e}")

    def close(self):
        """
        Clean up the Qwen client connection.
        Attempts to gracefully close the connection and provides CLI instructions
        if the programmatic close fails.
        """
        try:
            # Attempt to clean up the client instance
            if hasattr(self.client, '_session'):
                self.client._session.close()
            self.client = None
            logger.info("Qwen client connection closed.")
        except Exception as e:
            logger.warning(f"Failed to close Qwen client programmatically: {e}")
            logger.info("To completely stop Qwen, run in terminal:")
            logger.info("  ollama serve --stop")


class DeepseekModelLocal(LModel):
    """Qwen API implementation."""
    
    def _initialize_client(self) -> None:
        try:
            self.client = ollama.Client(host=self.config.ollama_host)
        except Exception as e:
            logger.warning(f"init qwen failed: {e}")
            raise ValueError("init qwen failed")
    
    def execute(self, messages: List[Dict[str, str]], 
                schema_model: Optional[Type[BaseModel]] = None) -> BaseModel:
        for attempt in range(self.config.max_retries):
            try:
                completion = self.client.chat(
                    model=self.model_name,
                    messages=messages,
                    options={"num_ctx": 5120, "timeout": self.config.timeout},
                    format=(schema_model or Output.StructureAnswer).model_json_schema(),
                )
                return (schema_model or Output.StructureAnswer).model_validate_json(
                    completion['message']['content']
                )
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    raise e
                logger.warning(f"Attempt {attempt + 1} failed: {e}")

    def close(self):
        """
        Clean up the Qwen client connection.
        Attempts to gracefully close the connection and provides CLI instructions
        if the programmatic close fails.
        """
        try:
            # Attempt to clean up the client instance
            if hasattr(self.client, '_session'):
                self.client._session.close()
            self.client = None
            logger.info("Qwen client connection closed.")
        except Exception as e:
            logger.warning(f"Failed to close Qwen client programmatically: {e}")
            logger.info("To completely stop Qwen, run in terminal:")
            logger.info("  ollama serve --stop")


class DeepseekModel(LModel):
    """Deepseek API implementation."""
    
    def _initialize_client(self) -> None:
        api_key = get_api_key("DEEPSEEK")
        if not api_key:
            raise ValueError("Deepseek API key not found in environment variables")
        try:
            self.client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
        except Exception as e:
            logger.warning(f"init deepseek failed: {e}")
            raise ValueError("init deepseek failed")
    
    def execute(self, messages: List[Dict[str, str]], 
                schema_model: Optional[Type[BaseModel]] = None) -> BaseModel:
        for attempt in range(self.config.max_retries):
            try:
                tmp_agent = instructor.from_openai(self.client, mode=instructor.Mode.JSON)
                completion = tmp_agent.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    response_model=schema_model or Output.StructureAnswer,
                    timeout=self.config.timeout
                )
                return completion
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    raise e
                logger.warning(f"Attempt {attempt + 1} failed: {e}")

    def close(self):
        self.client.close()
        logger.info("Connection closed")